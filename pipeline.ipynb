{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b8e4b5-3946-4a2d-bb6b-cb018571f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(GCNEncoder, self).__init__()\n",
    "            self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "            self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "    \n",
    "        def forward(self, x, edge_index):\n",
    "            x = torch.relu(self.conv1(x, edge_index))\n",
    "            x = torch.dropout(x, p=0.2, train=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe8535b0-0ac1-4280-909f-4f5e750de4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from littleballoffur import RandomWalkWithRestartSampler\n",
    "from littleballoffur import ForestFireSampler\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.loader import ClusterData\n",
    "from torch_geometric.loader import ClusterLoader\n",
    "from sklearn.decomposition import NMF\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "class GAEPipeline:\n",
    "    def __init__(self, in_channels, out_channels, sampling_method='method1', preprocessing=True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.sampling_method_name = sampling_method\n",
    "        self.sampling_method = self._get_sampling_method(sampling_method)\n",
    "        self.preprocessing = preprocessing\n",
    "        self.encoder = GCNEncoder(in_channels=11, out_channels=32)\n",
    "        self.model = GAE(self.encoder)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_sampling_method(self, method_name):\n",
    "        \"\"\"Dynamically selects the sampling method.\"\"\"\n",
    "        if method_name == 'random_walk':\n",
    "            return self.random_walk\n",
    "        elif method_name == 'forest_fire':\n",
    "            return self.forest_fire\n",
    "        elif method_name == 'clusterGCN':\n",
    "            return self.cluster_GCN\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sampling method\")\n",
    "\n",
    "    def convert_node_labels_to_integers(self, graph):\n",
    "        \"\"\"\n",
    "        Convert the node labels of a NetworkX graph to integers.\n",
    "        \n",
    "        Parameters:\n",
    "        - graph: NetworkX graph with any type of node labels.\n",
    "        \n",
    "        Returns:\n",
    "        - A new NetworkX graph with node labels converted to integers.\n",
    "        \"\"\"\n",
    "        # Create a mapping from old labels to new ones (integers)\n",
    "        mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "        # Use relabel_nodes to create a new graph with integer labels\n",
    "        graph_int_labels = nx.relabel_nodes(graph, mapping)\n",
    "        \n",
    "        return graph_int_labels\n",
    "\n",
    "    def preprocess_features(self, features):\n",
    "        # Convert features to a numeric format, handle non-numeric cases\n",
    "        processed_features = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                # convert the features to float\n",
    "                processed_features.append(float(feature))\n",
    "            except ValueError:\n",
    "                # Handle non-numeric feature (could implement encoding here)\n",
    "                processed_features.append(0.0)  # Using 0.0 as a placeholder\n",
    "        return processed_features\n",
    "\n",
    "    def normalize_features(self, features):\n",
    "        features = np.array(features)\n",
    "        mean = features.mean(axis=0, keepdims=True)\n",
    "        std = features.std(axis=0, keepdims=True)\n",
    "        # Avoid division by zero\n",
    "        std[std == 0] = 1\n",
    "        normalized_features = (features - mean) / std\n",
    "        return normalized_features.tolist()\n",
    "\n",
    "    def from_networkx_to_torch_geometric(self, G):\n",
    "        # Convert node indices to a continuous range\n",
    "        mapping = {k: i for i, k in enumerate(G.nodes())}\n",
    "        edges = torch.tensor([list(map(mapping.get, edge)) for edge in G.edges()], dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        if G.nodes():\n",
    "            # Extract a sample node to get feature keys (assumes at least one node exists)\n",
    "            sample_features = next(iter(G.nodes(data=True)))[1]\n",
    "            feature_keys = list(sample_features.keys())\n",
    "            \n",
    "            # Extract and preprocess features for all nodes\n",
    "            features = []\n",
    "            for _, node_features in G.nodes(data=True):\n",
    "                node_feature_values = [node_features.get(key, 0) for key in feature_keys]\n",
    "                processed_features = self.preprocess_features(node_feature_values)\n",
    "                features.append(processed_features)\n",
    "            \n",
    "            # Normalize features\n",
    "            features = self.normalize_features(features)\n",
    "        else:\n",
    "            # Default to a single feature of 0 if no nodes or features\n",
    "            features = [[0]]\n",
    "    \n",
    "        # Convert features to a tensor\n",
    "        x = torch.tensor(features, dtype=torch.float)\n",
    "    \n",
    "        # Create the Data object\n",
    "        data = Data(x=x, edge_index=edges)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def random_walk(self, graph):\n",
    "        graph = self.convert_node_labels_to_integers(graph)\n",
    "        model = RandomWalkWithRestartSampler(10000)\n",
    "        new_graph = model.sample(graph)\n",
    "        return new_graph\n",
    "\n",
    "    def forest_fire(self, graph):\n",
    "        graph = self.convert_node_labels_to_integers(graph)\n",
    "        model = ForestFireSampler(1000)\n",
    "        new_graph = model.sample(graph)\n",
    "        return new_graph\n",
    "\n",
    "    def cluster_GCN(self, data):\n",
    "        # Implement your third graph sampling algorithm here\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # data = data.to(device)\n",
    "        torch.manual_seed(12345)\n",
    "        # Prepare cluster data\n",
    "        cluster_data = ClusterData(data, num_parts=8, recursive=True) \n",
    "        # Create a loader to iterate over clusters\n",
    "        loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True)  \n",
    "        \n",
    "        print()\n",
    "        total_num_nodes = 0\n",
    "        for step, sub_data in enumerate(loader):\n",
    "            print(f'Step {step + 1}:')\n",
    "            print('=======')\n",
    "            print(f'Number of nodes in the current batch: {sub_data.num_nodes}')\n",
    "            print(sub_data)\n",
    "            print()\n",
    "            total_num_nodes += sub_data.num_nodes\n",
    "        \n",
    "        print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')\n",
    "        \n",
    "        return loader\n",
    "\n",
    "    \n",
    "\n",
    "    def preprocess_graph(self, graph):\n",
    "        # Placeholder function for preprocessing\n",
    "        # Implement your preprocessing logic here\n",
    "        data = self.from_networkx_to_torch_geometric(graph)\n",
    "        return data\n",
    "\n",
    "    def train(self, graph, epochs=500):\n",
    "        # Preprocessing\n",
    "\n",
    "        \n",
    "        # Graph Sampling\n",
    "        sampled_subgraph = self.sampling_method(graph)\n",
    "\n",
    "        if self.preprocessing:\n",
    "            data = self.preprocess_graph(sampled_subgraph)\n",
    "        \n",
    "        # data = Data(edge_index=sampled_subgraph.edge_index, x=sampled_subgraph.x)\n",
    "        \n",
    "        # Model Training\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            z = self.model.encode(data.x, data.edge_index)\n",
    "            loss = self.model.recon_loss(z, data.edge_index)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "            \n",
    "        torch.save(z, f'{self.sampling_method_name}_embedding.pt')\n",
    "\n",
    "\n",
    "    def train_clusterGCN(self, graph, epochs=500):\n",
    "        # Preprocessing\n",
    "        data = self.preprocess_graph(graph)\n",
    "        # Graph Sampling\n",
    "        loader = self.cluster_GCN(data)\n",
    "\n",
    "\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):  \n",
    "            total_loss = 0\n",
    "            for batch_idx, cluster in enumerate(loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                z = self.model.encode(cluster.x, cluster.edge_index)\n",
    "                loss = self.model.recon_loss(z, cluster.edge_index)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "                # total_loss += loss.item()\n",
    "            \n",
    "            # avg_loss = total_loss / len(loader)\n",
    "                print(f\"Epoch {epoch+1}, Average Loss: {loss.item()}\")\n",
    "                \n",
    "        torch.save(z, f'{self.sampling_method_name}_embedding.pt')\n",
    "\n",
    "\n",
    "    def non_negative_matrix_factorization(self, graph):\n",
    "        subgraph = self.sampling_method(graph)\n",
    "        \n",
    "        A = nx.to_numpy_array(subgraph)\n",
    "        print(A.shape)\n",
    "        model = NMF(n_components=2, init='random', random_state=0)\n",
    "        \n",
    "        # Fit the model to the adjacency matrix and transform\n",
    "        W = model.fit_transform(A)  # Basis matrix (features)\n",
    "        H = model.components_  # Coefficient matrix (components)\n",
    "        return W, H\n",
    "\n",
    "    def svd(self, graph):\n",
    "        subgraph = self.sampling_method(graph)\n",
    "        A = nx.to_numpy_array(subgraph)\n",
    "        U, S, Vt = svd(A, full_matrices=True)\n",
    "        return U, S, Vt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b07b8-798d-4f31-975c-b3a7173a21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_graphml(\"aggregated_proteins_v30.graphml\")\n",
    "pipeline = GAEPipeline(in_channels=11, out_channels=32, sampling_method='forest_fire')\n",
    "pipeline.train(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa0e36-419a-4aae-b81c-e5815683d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_graphml(\"aggregated_proteins_v30.graphml\")\n",
    "pipeline = GAEPipeline(in_channels=11, out_channels=32, sampling_method='clusterGCN')\n",
    "pipeline.train_clusterGCN(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34667653-55ea-43f6-a893-88815763a51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.07126714e-01,  1.03663346e-01, -1.74306127e-17, ...,\n",
       "          8.02817288e-14, -1.47232496e-13,  6.22626503e-14],\n",
       "        [-6.96964805e-03, -7.20250236e-03,  1.37538882e-17, ...,\n",
       "          5.09435538e-01, -2.25257230e-02, -4.59647577e-02],\n",
       "        [-6.96964805e-03, -7.20250236e-03,  4.47387511e-17, ...,\n",
       "         -7.15199696e-03, -1.04293884e-03,  8.17146610e-04],\n",
       "        ...,\n",
       "        [-6.96964805e-03, -7.20250236e-03,  7.99728626e-17, ...,\n",
       "          5.66758230e-02, -4.30608635e-02, -8.94499976e-03],\n",
       "        [-6.96964805e-03, -7.20250236e-03,  7.98561014e-17, ...,\n",
       "          5.66758230e-02, -4.30608635e-02, -8.94499976e-03],\n",
       "        [-6.96964805e-03, -7.20250236e-03,  7.96159510e-17, ...,\n",
       "          5.66758230e-02, -4.30608635e-02, -8.94499976e-03]]),\n",
       " array([6.91670815e+02, 6.47670815e+02, 1.00000000e+00, ...,\n",
       "        2.31160422e-15, 1.07535966e-15, 9.53098613e-16]),\n",
       " array([[-1.07126714e-01, -6.96964805e-03, -6.96964805e-03, ...,\n",
       "         -6.96964805e-03, -6.96964805e-03, -6.96964805e-03],\n",
       "        [-1.03663346e-01,  7.20250236e-03,  7.20250236e-03, ...,\n",
       "          7.20250236e-03,  7.20250236e-03,  7.20250236e-03],\n",
       "        [ 2.29713161e-17,  2.05743955e-18,  2.00173726e-18, ...,\n",
       "          7.61720936e-18, -7.12395719e-16, -7.12443471e-16],\n",
       "        ...,\n",
       "        [ 6.71059864e-14,  2.56274863e-01, -8.02974368e-02, ...,\n",
       "          1.08262046e-04,  1.20607478e-03,  1.20607478e-03],\n",
       "        [ 0.00000000e+00, -2.26761535e-01,  7.38436893e-02, ...,\n",
       "          8.59895085e-05,  5.76300719e-03,  5.76300719e-03],\n",
       "        [ 1.01230746e-12, -2.44894437e-03,  5.67547367e-03, ...,\n",
       "          6.61837804e-06, -2.21648676e-04, -2.21648676e-04]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = nx.read_graphml(\"aggregated_proteins_v30.graphml\")\n",
    "pipeline = GAEPipeline(in_channels=11, out_channels=32, sampling_method='random_walk')\n",
    "pipeline.svd(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f5a2a-b02a-42b0-a186-17e1ebefe929",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b00dcaa8-8856-416f-852e-468d270243be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.51355222e-06 4.20314926e-02 4.20314947e-02 ... 4.20315017e-02\n",
      "  4.20315060e-02 4.20314950e-02]\n",
      " [1.60202193e+00 4.41530137e-05 4.40754106e-05 ... 4.38195937e-05\n",
      "  4.36605349e-05 4.40637424e-05]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "A_sparse = csr_matrix(A)\n",
    "\n",
    "model = NMF(n_components=2, init='random', random_state=42, solver='mu')\n",
    "\n",
    "\n",
    "W = model.fit_transform(A_sparse)  # Basis matrix (features)\n",
    "H = model.components_  # Coefficient matrix (components)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64459606-cdc1-49b1-a127-27d587707bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNNenv",
   "language": "python",
   "name": "gnnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
